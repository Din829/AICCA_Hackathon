"""
Deepfakeæ£€æµ‹å·¥å…· - æ£€æµ‹é¢éƒ¨æ›¿æ¢ã€è¯­éŸ³å…‹éš†ç­‰æ·±åº¦ä¼ªé€ å†…å®¹
Deepfake Detection - ä½¿ç”¨å¤šç§æŠ€æœ¯æ£€æµ‹æ·±åº¦ä¼ªé€ å†…å®¹
"""

import asyncio
import os
import json
from typing import Dict, Any, Optional, Union, Callable, List
from pathlib import Path
import hashlib
from datetime import datetime
import tempfile
import uuid

# ä½¿ç”¨æˆç†Ÿçš„dbrheoæ¥å£
from dbrheo.types.tool_types import ToolResult
from dbrheo.types.core_types import AbortSignal
from dbrheo.tools.base import DatabaseTool, DatabaseConfirmationDetails
from dbrheo.config.base import DatabaseConfig
from dbrheo.utils.debug_logger import log_info


class DeepfakeDetector(DatabaseTool):
    """
    Deepfakeæ£€æµ‹å·¥å…·
    ä¸“é—¨æ£€æµ‹é¢éƒ¨æ›¿æ¢ã€è¯­éŸ³å…‹éš†ã€èº«ä½“æ›¿æ¢ç­‰æ·±åº¦ä¼ªé€ æŠ€æœ¯
    """
    
    def __init__(self, config: DatabaseConfig, i18n=None):
        self._i18n = i18n
        super().__init__(
            name="deepfake_detector",
            display_name=self._('deepfake_name', default="Deepfake Detector") if i18n else "Deepfake Detector",
            description="Deepfake and face swap specialist. Focuses on detecting facial manipulation, face replacement, and voice cloning. For images: detects face swaps only. For videos: supports both facial and general AI detection. For audio: detects voice synthesis. Use image_verify for general AI image detection.",
            parameter_schema={
                "type": "object",
                "properties": {
                    "media_path": {
                        "type": "string",
                        "description": "Path or URL to media file (image/video/audio)"
                    },
                    "media_type": {
                        "type": "string",
                        "enum": ["image", "video", "audio", "auto"],
                        "description": "Media type (auto-detect if not specified)",
                        "default": "auto"
                    },
                    "detection_focus": {
                        "type": "string",
                        "enum": ["auto", "facial", "general", "comprehensive"],
                        "description": "Detection focus: facial=face manipulation, general=AI content, comprehensive=both, auto=intelligent selection",
                        "default": "auto"
                    },
                    "analysis_depth": {
                        "type": "string",
                        "enum": ["quick", "standard", "forensic"],
                        "description": "Analysis depth: quick=fast scan, standard=balanced, forensic=detailed",
                        "default": "standard"
                    },
                    "check_metadata": {
                        "type": "boolean",
                        "description": "Check for suspicious metadata and encoder signatures",
                        "default": True
                    },
                    "biological_analysis": {
                        "type": "boolean",
                        "description": "Analyze biological features (eye blink, facial symmetry)",
                        "default": True
                    }
                },
                "required": ["media_path"]
            },
            is_output_markdown=True,
            can_update_output=True,
            should_summarize_display=True,
            i18n=i18n
        )
        self.config = config
        
        # APIé…ç½®
        self.api_config = self._initialize_api_config()
        
        # æ”¯æŒçš„åª’ä½“æ ¼å¼
        self.supported_formats = {
            'image': {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp'},
            'video': {'.mp4', '.avi', '.mov', '.mkv', '.webm', '.flv'},
            'audio': {'.mp3', '.wav', '.m4a', '.aac', '.ogg', '.flac'}
        }
        
        # æ£€æŸ¥ffmpegå¯ç”¨æ€§
        self.ffmpeg_available = self._check_ffmpeg_availability()
    
    def _initialize_api_config(self) -> Dict[str, Any]:
        """åˆå§‹åŒ–APIé…ç½®"""
        config = {}
        
        # Sightengineé…ç½®ï¼ˆä¸image_verify_toolå…±ç”¨å‡­è¯ï¼‰
        # æ¸…ç†ç¯å¢ƒå˜é‡ä¸­çš„æ¢è¡Œç¬¦å’Œç©ºç™½å­—ç¬¦ï¼ˆéƒ¨ç½²ç¯å¢ƒSecret Manageré—®é¢˜ä¿®å¤ï¼‰
        api_user = os.environ.get("SIGHTENGINE_API_USER") or self.config.get("sightengine_api_user")
        api_secret = os.environ.get("SIGHTENGINE_API_SECRET") or self.config.get("sightengine_api_secret")

        config["sightengine"] = {
            "enabled": self.config.get("sightengine_enabled", True),  # ä½¿ç”¨ç›¸åŒçš„é…ç½®é”®
            "api_user": api_user.strip() if api_user else None,
            "api_secret": api_secret.strip() if api_secret else None,
            "endpoint": "https://api.sightengine.com/1.0/check.json",  # å›¾åƒç«¯ç‚¹
            "video_sync_endpoint": "https://api.sightengine.com/1.0/video/check-sync.json",  # åŒæ­¥è§†é¢‘ç«¯ç‚¹ï¼ˆ<=1åˆ†é’Ÿï¼‰
            "video_async_endpoint": "https://api.sightengine.com/1.0/video/check.json"  # å¼‚æ­¥è§†é¢‘ç«¯ç‚¹ï¼ˆ>1åˆ†é’Ÿï¼‰
        }
        
        # Resemble AIé…ç½®ï¼ˆç”¨äºéŸ³é¢‘deepfakeæ£€æµ‹ï¼‰
        config["resemble"] = {
            "enabled": self.config.get("deepfake_resemble_enabled", False),
            "api_key": os.environ.get("RESEMBLE_API_KEY") or self.config.get("resemble_api_key"),
            "endpoint": "https://api.resemble.ai/v1/detect"
        }
        
        # æœ¬åœ°åˆ†æé…ç½®ï¼ˆç®€åŒ–ï¼‰
        config["local_analysis"] = {
            "enabled": True,
            "temporal_analysis": True,
            "face_landmarks": True
        }
        
        return config
    
    def _check_ffmpeg_availability(self) -> bool:
        """æ£€æŸ¥ffmpegæ˜¯å¦å¯ç”¨"""
        import subprocess
        import platform
        
        log_info("DeepfakeDetector", f"[DEBUG] Checking FFmpeg availability on {platform.system()}")
        
        # Windowsç‰¹å®šçš„å¸¸è§FFmpegè·¯å¾„
        windows_ffmpeg_paths = [
            r"C:\Program Files\ffmpeg\bin\ffmpeg.exe",
            r"C:\Program Files\ffmpeg-7.1.1-full_build\bin\ffmpeg.exe",
            r"C:\Program Files (x86)\ffmpeg\bin\ffmpeg.exe",
            r"C:\ffmpeg\bin\ffmpeg.exe",
            r"D:\ffmpeg\bin\ffmpeg.exe"
        ]
        
        # é¦–å…ˆå°è¯•ç›´æ¥è°ƒç”¨ffmpegï¼ˆå¦‚æœåœ¨PATHä¸­ï¼‰
        try:
            log_info("DeepfakeDetector", "[DEBUG] Trying to run 'ffmpeg -version' from PATH")
            result = subprocess.run(
                ['ffmpeg', '-version'],
                capture_output=True,
                text=True,
                timeout=2
            )
            log_info("DeepfakeDetector", f"[DEBUG] ffmpeg from PATH returncode: {result.returncode}")
            if result.returncode == 0:
                log_info("DeepfakeDetector", f"[DEBUG] ffmpeg output: {result.stdout[:100]}")
                log_info("DeepfakeDetector", "âœ“ ffmpeg is available in PATH")
                self.ffmpeg_command = 'ffmpeg'
                return True
        except Exception as e:
            log_info("DeepfakeDetector", f"[DEBUG] Failed to run ffmpeg from PATH: {e}")
        
        # Windowsç³»ç»Ÿï¼šå°è¯•å¸¸è§è·¯å¾„
        if platform.system() == 'Windows':
            log_info("DeepfakeDetector", "[DEBUG] Windows detected, checking common FFmpeg paths")
            for ffmpeg_path in windows_ffmpeg_paths:
                log_info("DeepfakeDetector", f"[DEBUG] Checking: {ffmpeg_path}")
                if os.path.exists(ffmpeg_path):
                    try:
                        log_info("DeepfakeDetector", f"[DEBUG] File exists, testing: {ffmpeg_path}")
                        result = subprocess.run(
                            [ffmpeg_path, '-version'],
                            capture_output=True,
                            text=True,
                            timeout=2
                        )
                        log_info("DeepfakeDetector", f"[DEBUG] {ffmpeg_path} returncode: {result.returncode}")
                        if result.returncode == 0:
                            log_info("DeepfakeDetector", f"âœ“ ffmpeg found at: {ffmpeg_path}")
                            self.ffmpeg_command = ffmpeg_path
                            return True
                    except Exception as e:
                        log_info("DeepfakeDetector", f"[DEBUG] Failed to run {ffmpeg_path}: {e}")
                else:
                    log_info("DeepfakeDetector", f"[DEBUG] Not found: {ffmpeg_path}")
        
        log_info("DeepfakeDetector", "âœ— ffmpeg not found - video analysis will be limited")
        self.ffmpeg_command = 'ffmpeg'  # é»˜è®¤å€¼
        return False
    
    def validate_tool_params(self, params: Dict[str, Any]) -> Optional[str]:
        """éªŒè¯å‚æ•°"""
        params = self._normalize_params(params)
        
        media_path = params.get("media_path", "")
        if not media_path:
            return "Media path cannot be empty"
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼ˆæœ¬åœ°æ–‡ä»¶ï¼‰
        if not media_path.startswith(('http://', 'https://')):
            path = Path(media_path)
            if not path.exists():
                return f"File not found: {media_path}"
            
            # æ£€æŸ¥æ–‡ä»¶ç±»å‹
            valid_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.mp4', '.avi', '.mov', '.mp3', '.wav', '.m4a'}
            if path.suffix.lower() not in valid_extensions:
                return f"Unsupported media format: {path.suffix}. Supported: {', '.join(valid_extensions)}"
        
        return None
    
    def _determine_video_model(self, detection_focus: str, video_path: str) -> str:
        """æ™ºèƒ½å†³å®šä½¿ç”¨å“ªä¸ªæ¨¡å‹è¿›è¡Œè§†é¢‘åˆ†æ"""
        # å®Œå…¨ç”±Agenté€šè¿‡detection_focuså‚æ•°æ§åˆ¶ï¼Œæ— ä»»ä½•å…³é”®è¯åŒ¹é…
        # Agentä¼šæ ¹æ®ç”¨æˆ·éœ€æ±‚æ™ºèƒ½é€‰æ‹©åˆé€‚çš„æ¨¡å‹
        
        if detection_focus == "facial":
            # Agentæ˜ç¡®é€‰æ‹©äº†é¢éƒ¨æ£€æµ‹ï¼ˆç”¨æˆ·æ˜ç¡®æåˆ°äººè„¸ç›¸å…³ï¼‰
            return "deepfake"
        elif detection_focus == "general":
            # Agenté€‰æ‹©äº†é€šç”¨AIæ£€æµ‹ï¼ˆé»˜è®¤é€‰æ‹©ï¼‰
            return "genai"
        elif detection_focus == "comprehensive":
            # Agenté€‰æ‹©äº†å…¨é¢æ£€æµ‹ï¼ˆéœ€è¦ä¸¤ç§æ£€æµ‹ï¼‰
            return "comprehensive"
        elif detection_focus == "auto":
            # autoæ¨¡å¼ï¼šé»˜è®¤ä½¿ç”¨genaiï¼ˆé€šç”¨AIæ£€æµ‹ï¼‰
            # å› ä¸ºgenaiå¯ä»¥æ£€æµ‹æ‰€æœ‰ç±»å‹çš„AIç”Ÿæˆå†…å®¹
            # åªæœ‰ç”¨æˆ·æ˜ç¡®æåˆ°äººè„¸æ—¶Agentæ‰ä¼šé€‰æ‹©facial
            return "genai"  # é»˜è®¤genaiï¼Œæ£€æµ‹æ‰€æœ‰AIç”Ÿæˆå†…å®¹
        else:
            return "genai"  # æœªçŸ¥æƒ…å†µä¹Ÿé»˜è®¤genai
    
    def get_description(self, params: Dict[str, Any]) -> str:
        """è·å–æ“ä½œæè¿°"""
        media_path = params.get("media_path", "")
        media_type = params.get("media_type", "auto")
        analysis_depth = params.get("analysis_depth", "standard")
        
        # è·å–æ–‡ä»¶åï¼Œé™åˆ¶é•¿åº¦
        if media_path.startswith('file:'):
            filename = f"uploaded_{media_path[5:][:8]}"
        elif media_path.startswith(('http://', 'https://')):
            filename = media_path.split('/')[-1].split('?')[0][:50]
        else:
            filename = Path(media_path).name
        
        return f"Performing {analysis_depth} deepfake detection: {filename}"
    
    async def should_confirm_execute(
        self,
        params: Dict[str, Any],
        signal: AbortSignal
    ) -> Union[bool, DatabaseConfirmationDetails]:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦ç¡®è®¤"""
        # Deepfakeæ£€æµ‹é€šå¸¸ä¸éœ€è¦ç¡®è®¤
        return False
    
    async def execute(
        self,
        params: Dict[str, Any],
        signal: AbortSignal,
        update_output: Optional[Callable[[str], None]] = None
    ) -> ToolResult:
        """æ‰§è¡ŒDeepfakeæ£€æµ‹"""
        params = self._normalize_params(params)
        
        media_path = params.get("media_path", "")
        media_type = params.get("media_type", "auto")
        analysis_depth = params.get("analysis_depth", "standard")
        
        try:
            # å‡†å¤‡åª’ä½“æ–‡ä»¶
            if update_output:
                update_output("Loading media file...")
            
            # ç¡®å®šåª’ä½“ç±»å‹
            if media_type == "auto":
                media_type = self._detect_media_type(media_path)
            
            if update_output:
                update_output(f"Analyzing {media_type} for deepfakes (depth: {analysis_depth})...")
            
            results = {}
            
            # æ ¹æ®åª’ä½“ç±»å‹é€‰æ‹©æ£€æµ‹ç­–ç•¥
            if media_type == "image":
                # å›¾åƒdeepfakeæ£€æµ‹
                results = await self._detect_image_deepfake(
                    media_path, analysis_depth, 
                    params.get("check_metadata", True),
                    params.get("biological_analysis", True),
                    update_output
                )
            
            elif media_type == "video":
                # è§†é¢‘deepfakeæ£€æµ‹
                results = await self._detect_video_deepfake(
                    media_path, analysis_depth,
                    params.get("detection_focus", "auto"),
                    params.get("check_metadata", True),
                    update_output
                )
            
            elif media_type == "audio":
                # éŸ³é¢‘deepfakeæ£€æµ‹
                results = await self._detect_audio_deepfake(
                    media_path, analysis_depth,
                    update_output
                )
            
            else:
                return ToolResult(
                    error="Unsupported media type",
                    llm_content={"error": f"Unsupported media type: {media_type}"},
                    return_display=f"Unsupported media type: {media_type}"
                )
            
            # ç”Ÿæˆåˆ†ææŠ¥å‘Š
            report = self._generate_report(results, media_type, analysis_depth)
            
            # ç”Ÿæˆæ‘˜è¦
            summary = self._generate_summary(results, media_type)
            
            return ToolResult(
                summary=summary,
                llm_content=results,
                return_display=report
            )
            
        except Exception as e:
            error_msg = f"Deepfake detection failed: {str(e)}"
            log_info("DeepfakeDetector", f"Error: {e}")
            return ToolResult(
                error=error_msg,
                llm_content={"error": str(e)},
                return_display=f"[ERROR] {error_msg}"
            )
    
    def _detect_media_type(self, media_path: str) -> str:
        """è‡ªåŠ¨æ£€æµ‹åª’ä½“ç±»å‹"""
        if media_path.startswith(('http://', 'https://')):
            # URL - ä»æ‰©å±•åæ¨æ–­
            ext = '.' + media_path.split('.')[-1].lower().split('?')[0]
        elif media_path.startswith('file:'):
            # å¯¹äºfile:IDæ ¼å¼ï¼Œå°è¯•ä»file_storageè·å–åŸå§‹æ–‡ä»¶å
            try:
                # å°è¯•å¤šç§æ–¹å¼å¯¼å…¥file_storage
                try:
                    from packages.api.aicca_api import file_storage
                except ImportError:
                    try:
                        from api.aicca_api import file_storage
                    except ImportError:
                        import sys
                        import os
                        # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
                        root_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
                        sys.path.insert(0, os.path.join(root_dir, 'api'))
                        from aicca_api import file_storage
                file_id = media_path[5:]  # å»æ‰'file:'å‰ç¼€
                file_info = file_storage.get_file_info(file_id)
                if file_info and file_info.get('original_name'):
                    # ä»åŸå§‹æ–‡ä»¶åè·å–æ‰©å±•å
                    ext = Path(file_info['original_name']).suffix.lower()
                    log_info("DeepfakeDetector", f"Detected extension from original_name: {ext}")
                else:
                    # å¦‚æœæ²¡æœ‰åŸå§‹æ–‡ä»¶åï¼Œå°è¯•ä»content_typeæ¨æ–­
                    if file_info and file_info.get('content_type'):
                        content_type = file_info['content_type']
                        if 'video' in content_type:
                            ext = '.mp4'
                        elif 'image' in content_type:
                            ext = '.jpg'
                        elif 'audio' in content_type:
                            ext = '.mp3'
                        else:
                            ext = '.jpg'  # é»˜è®¤
                    else:
                        ext = '.jpg'  # é»˜è®¤
            except Exception as e:
                log_info("DeepfakeDetector", f"Failed to get file info for media type detection: {e}")
                ext = '.jpg'  # å‡ºé”™æ—¶é»˜è®¤ä¸ºå›¾åƒ
        else:
            ext = Path(media_path).suffix.lower()
        
        for media_type, extensions in self.supported_formats.items():
            if ext in extensions:
                return media_type
        
        return "image"  # é»˜è®¤
    
    async def _detect_image_deepfake(
        self, 
        image_path: str, 
        analysis_depth: str,
        check_metadata: bool,
        biological_analysis: bool,
        update_output: Optional[Callable] = None
    ) -> Dict[str, Any]:
        """æ£€æµ‹å›¾åƒdeepfake"""
        results = {"media_type": "image"}
        
        # 1. Sightengine APIæ£€æµ‹
        if self.api_config["sightengine"]["enabled"]:
            if update_output:
                update_output("Analyzing with Sightengine deepfake detector...")
            
            sightengine_result = await self._call_sightengine_image_api(image_path)
            results["sightengine_analysis"] = sightengine_result
        
        # 2. æœ¬åœ°åˆ†æï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.api_config["local_analysis"]["enabled"] and analysis_depth != "quick":
            if update_output:
                update_output("Performing local forensic analysis...")
            
            local_result = await self._local_image_analysis(image_path, check_metadata)
            results["local_analysis"] = local_result
        
        # 3. ç”Ÿç‰©ç‰¹å¾åˆ†æï¼ˆå¦‚æœå¯ç”¨ä¸”æ˜¯æ·±åº¦åˆ†æï¼‰
        if biological_analysis and analysis_depth == "forensic":
            if update_output:
                update_output("Analyzing biological features...")
            
            bio_result = await self._biological_feature_analysis(image_path)
            results["biological_analysis"] = bio_result
        
        return results
    
    async def _detect_video_deepfake(
        self,
        video_path: str,
        analysis_depth: str,
        detection_focus: str,
        check_metadata: bool,
        update_output: Optional[Callable] = None
    ) -> Dict[str, Any]:
        """æ£€æµ‹è§†é¢‘deepfake - æ”¯æŒfacialå’Œgeneralä¸¤ç§æ¨¡å¼"""
        results = {"media_type": "video"}
        
        # 1. Sightengineè§†é¢‘æ£€æµ‹
        if self.api_config["sightengine"]["enabled"]:
            if update_output:
                ffmpeg_status = "available" if self.ffmpeg_available else "not available"
                update_output(f"Analyzing video (ffmpeg: {ffmpeg_status})...")
            
            # å†³å®šä½¿ç”¨å“ªä¸ªæ¨¡å‹
            use_model = self._determine_video_model(detection_focus, video_path)
            log_info("DeepfakeDetector", f"Using model: {use_model} for video analysis")
            
            # æ ¹æ®æ¨¡å‹é€‰æ‹©ä¸åŒçš„å¤„ç†ç­–ç•¥
            if use_model == "deepfake":
                # ä½¿ç”¨deepfakeæ¨¡å‹ï¼ˆé¢éƒ¨æ£€æµ‹ï¼‰- æ”¯æŒåŒæ­¥API
                video_duration = await self._get_video_duration(video_path)
                
                if video_duration and video_duration <= 60:
                    # çŸ­è§†é¢‘ï¼šä½¿ç”¨åŒæ­¥API (check-sync.json)
                    if update_output:
                        update_output(f"Using sync API for {video_duration}s video (facial detection)...")
                    video_result = await self._call_video_sync_api(video_path, model="deepfake")
                else:
                    # é•¿è§†é¢‘ï¼šæ£€æŸ¥æ˜¯å¦æœ‰callback_url
                    callback_url = self.config.get("sightengine_callback_url")
                    if callback_url:
                        # æœ‰callbackï¼šä½¿ç”¨å¼‚æ­¥API
                        if update_output:
                            update_output("Using async API with callback (facial detection)...")
                        video_result = await self._call_video_async_api(video_path, callback_url, model="deepfake")
                    else:
                        # æ— callbackï¼šé™çº§åˆ°å…³é”®å¸§åˆ†æ
                        if update_output:
                            update_output("No callback URL, using keyframe analysis (facial detection)...")
                        video_result = await self._analyze_video_keyframes_with_model(video_path, model="deepfake")
            
            elif use_model == "genai":
                # ä½¿ç”¨genaiæ¨¡å‹ï¼ˆé€šç”¨AIæ£€æµ‹ï¼‰- åªèƒ½é€šè¿‡å…³é”®å¸§åˆ†æ
                if update_output:
                    update_output("Using frame extraction for general AI detection...")
                video_result = await self._analyze_video_keyframes_with_model(video_path, model="genai")
            
            elif use_model == "comprehensive":
                # ç»¼åˆæ£€æµ‹ï¼šåŒæ—¶ä½¿ç”¨ä¸¤ä¸ªæ¨¡å‹
                if update_output:
                    update_output("Performing comprehensive analysis (facial + general AI)...")
                
                # å…ˆåšdeepfakeæ£€æµ‹
                deepfake_result = await self._analyze_video_keyframes_with_model(video_path, model="deepfake")
                # å†åšgenaiæ£€æµ‹
                genai_result = await self._analyze_video_keyframes_with_model(video_path, model="genai")
                
                video_result = {
                    "comprehensive_mode": True,
                    "facial_analysis": deepfake_result,
                    "general_ai_analysis": genai_result,
                    "combined_assessment": self._combine_assessments(deepfake_result, genai_result)
                }
            else:
                video_result = {"error": f"Unknown model: {use_model}"}
            
            results["video_analysis"] = video_result
            results["model_used"] = use_model
        
        # 2. æ—¶åºä¸€è‡´æ€§åˆ†æ
        if self.api_config["local_analysis"]["temporal_analysis"] and analysis_depth != "quick":
            if update_output:
                update_output("Analyzing temporal consistency...")
            
            temporal_result = await self._temporal_consistency_analysis(video_path)
            results["temporal_analysis"] = temporal_result
        
        # 3. å…ƒæ•°æ®æ£€æŸ¥
        if check_metadata:
            log_info("DeepfakeDetector", f"[DEBUG] Metadata check enabled: {check_metadata}")
            if update_output:
                update_output("Checking video metadata...")
            
            metadata_result = await self._check_video_metadata(video_path)
            log_info("DeepfakeDetector", f"[DEBUG] Metadata result status: {metadata_result.get('status', 'unknown')}")
            results["metadata_analysis"] = metadata_result
        else:
            log_info("DeepfakeDetector", f"[DEBUG] Metadata check disabled: check_metadata={check_metadata}")
        
        return results
    
    async def _detect_audio_deepfake(
        self,
        audio_path: str,
        analysis_depth: str,
        update_output: Optional[Callable] = None
    ) -> Dict[str, Any]:
        """æ£€æµ‹éŸ³é¢‘deepfake"""
        results = {
            "media_type": "audio",
            "note": "Audio deepfake detection is limited. Consider using specialized audio analysis tools.",
            "alternatives": ["Resemble AI", "Pindrop", "Deepware Scanner"]
        }
        
        # Resemble AIæ£€æµ‹ï¼ˆå¦‚æœé…ç½®ï¼‰
        if self.api_config["resemble"]["enabled"] and self.api_config["resemble"]["api_key"]:
            if update_output:
                update_output("Analyzing with Resemble AI...")
            
            resemble_result = await self._call_resemble_api(audio_path)
            results["resemble_analysis"] = resemble_result
        else:
            # åŸºç¡€åˆ†æ
            if update_output:
                update_output("Performing basic audio analysis...")
            
            basic_result = await self._basic_audio_analysis(audio_path)
            results["basic_analysis"] = basic_result
        
        return results
    
    async def _call_sightengine_image_api(self, image_path: str) -> Dict[str, Any]:
        """è°ƒç”¨Sightengineå›¾åƒdeepfakeæ£€æµ‹API"""
        try:
            import urllib.request
            import urllib.parse
            import urllib.error

            api_user = self.api_config["sightengine"]["api_user"]
            api_secret = self.api_config["sightengine"]["api_secret"]

            # æ·»åŠ è¯¦ç»†çš„APIè°ƒè¯•æ—¥å¿—
            log_info("DeepfakeDetector", f"ğŸ”‘ API Credentials Check:")
            log_info("DeepfakeDetector", f"  - User: {api_user[:3] if api_user else 'None'}...{api_user[-3:] if api_user and len(api_user) > 6 else ''}")
            log_info("DeepfakeDetector", f"  - Secret: {api_secret[:3] if api_secret else 'None'}...{api_secret[-3:] if api_secret and len(api_secret) > 6 else ''}")
            log_info("DeepfakeDetector", f"  - Endpoint: {self.api_config['sightengine']['endpoint']}")

            if not api_user or not api_secret:
                log_info("DeepfakeDetector", "Sightengine API credentials not configured")
                return {
                    "error": "Sightengine API credentials not configured",
                    "help": "Set SIGHTENGINE_API_USER and SIGHTENGINE_API_SECRET environment variables"
                }
            
            # å¤„ç†file:IDæ ¼å¼
            actual_path = image_path
            if image_path.startswith('file:'):
                try:
                    from aicca.utils.content_loader import ContentLoader
                    loader = ContentLoader()
                    actual_path, metadata = await loader.load_content(image_path)
                    log_info("DeepfakeDetector", f"Resolved file:ID to {actual_path}")
                except Exception as e:
                    log_info("DeepfakeDetector", f"ContentLoader failed: {e}, using fallback")
                    actual_path = image_path[5:]  # é™çº§å¤„ç†
            
            # æ„å»ºè¯·æ±‚å‚æ•°
            params = {
                'models': 'deepfake',  # ä½¿ç”¨deepfakeæ¨¡å‹è€Œä¸æ˜¯genai
                'api_user': api_user,
                'api_secret': api_secret
            }
            
            # æ ¹æ®è¾“å…¥ç±»å‹é€‰æ‹©å‚æ•°
            if actual_path.startswith(('http://', 'https://')):
                params['url'] = actual_path

                # æ·»åŠ è¯·æ±‚è°ƒè¯•æ—¥å¿—
                log_info("DeepfakeDetector", f"ğŸ“¤ Sending URL request to Sightengine:")
                log_info("DeepfakeDetector", f"  - URL: {actual_path}")
                log_info("DeepfakeDetector", f"  - Params: {params}")

                # URLæ–¹å¼
                data = urllib.parse.urlencode(params).encode()
                req = urllib.request.Request(
                    self.api_config["sightengine"]["endpoint"],
                    data=data
                )

                with urllib.request.urlopen(req, timeout=30) as response:
                    result = json.loads(response.read().decode())
                    log_info("DeepfakeDetector", f"ğŸ“¥ Received response: {result}")
            else:
                # æœ¬åœ°æ–‡ä»¶éœ€è¦ä¸Šä¼ 
                log_info("DeepfakeDetector", f"ğŸ“¤ Sending file upload request to Sightengine:")
                log_info("DeepfakeDetector", f"  - File: {actual_path}")
                log_info("DeepfakeDetector", f"  - Params: {params}")

                import requests
                with open(actual_path, 'rb') as f:
                    files = {'media': f}
                    response = requests.post(
                        self.api_config["sightengine"]["endpoint"],
                        files=files,
                        data=params,
                        timeout=self.api_config["sightengine"].get("timeout", 30)
                    )
                    result = response.json()
                    log_info("DeepfakeDetector", f"ğŸ“¥ Received response: {result}")
            
            # æå–deepfakeåˆ†æ•°
            if "type" in result and "deepfake" in result["type"]:
                log_info("DeepfakeDetector", f"Sightengine deepfake score: {result['type']['deepfake']}")
                return {
                    "deepfake_score": result["type"]["deepfake"],
                    "status": result.get("status", "unknown"),
                    "operations": result.get("request", {}).get("operations", 1)
                }
            elif "error" in result:
                log_info("DeepfakeDetector", f"Sightengine API error: {result.get('error', {}).get('message', 'Unknown')}")
                return {
                    "error": result.get("error", {}).get("message", "API error"),
                    "code": result.get("error", {}).get("code")
                }
            else:
                return result  # è¿”å›åŸå§‹ç»“æœ
                
        except urllib.error.HTTPError as e:
            error_body = e.read().decode() if e.fp else ""
            log_info("DeepfakeDetector", f"HTTP Error {e.code}: {error_body}")
            
            # è§£æé”™è¯¯å“åº”
            try:
                error_data = json.loads(error_body)
                error_msg = error_data.get("error", {}).get("message", f"HTTP {e.code}")
            except:
                error_msg = f"HTTP Error {e.code}: {e.reason}"
            
            return {
                "error": error_msg,
                "http_code": e.code,
                "url": image_path if image_path.startswith(('http://', 'https://')) else "local_file"
            }
        except Exception as e:
            log_info("DeepfakeDetector", f"Sightengine API exception: {e}")
            return {
                "error": f"Sightengine API error: {str(e)}",
                "exception_type": type(e).__name__
            }
    
    async def _get_video_duration(self, video_path: str) -> Optional[float]:
        """è·å–è§†é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰- ä½¿ç”¨ffmpeg/ffprobe"""
        try:
            import subprocess
            import json as json_module
            
            # å¤„ç†file:IDæ ¼å¼
            actual_path = video_path
            if video_path.startswith('file:'):
                try:
                    from aicca.utils.content_loader import ContentLoader
                    loader = ContentLoader()
                    actual_path, metadata = await loader.load_content(video_path)
                except Exception as e:
                    log_info("DeepfakeDetector", f"ContentLoader failed in get_video_duration: {e}")
                    actual_path = video_path[5:]  # é™çº§å¤„ç†
            
            # å¯¹äºURLï¼Œä½¿ç”¨ffprobeè·å–æ—¶é•¿
            if actual_path.startswith(('http://', 'https://')):
                # ffprobeå¯ä»¥ç›´æ¥åˆ†æURL
                ffprobe_cmd = getattr(self, 'ffmpeg_command', 'ffmpeg').replace('ffmpeg', 'ffprobe')
                cmd = [
                    ffprobe_cmd,
                    '-v', 'quiet',
                    '-print_format', 'json',
                    '-show_format',
                    '-show_streams',
                    video_path
                ]
                
                try:
                    result = subprocess.run(
                        cmd,
                        capture_output=True,
                        text=True,
                        timeout=10  # URLå¯èƒ½éœ€è¦æ›´é•¿æ—¶é—´
                    )
                    
                    if result.returncode == 0:
                        data = json_module.loads(result.stdout)
                        # ä»formatä¸­è·å–æ—¶é•¿
                        if 'format' in data and 'duration' in data['format']:
                            duration = float(data['format']['duration'])
                            log_info("DeepfakeDetector", f"Video duration (URL): {duration:.2f}s")
                            return duration
                    else:
                        log_info("DeepfakeDetector", f"ffprobe failed for URL: {result.stderr}")
                        
                except subprocess.TimeoutExpired:
                    log_info("DeepfakeDetector", "ffprobe timeout for URL")
                except FileNotFoundError:
                    log_info("DeepfakeDetector", "ffprobe not found, please install ffmpeg")
                except Exception as e:
                    log_info("DeepfakeDetector", f"ffprobe error for URL: {e}")
            
            # å¯¹äºæœ¬åœ°æ–‡ä»¶ï¼Œä¼˜å…ˆä½¿ç”¨ffprobe
            else:
                # æ–¹æ³•1: ä½¿ç”¨ffprobeï¼ˆæœ€å‡†ç¡®ï¼‰
                ffprobe_cmd = getattr(self, 'ffmpeg_command', 'ffmpeg').replace('ffmpeg', 'ffprobe')
                cmd = [
                    ffprobe_cmd,
                    '-v', 'quiet',
                    '-print_format', 'json',
                    '-show_format',
                    video_path
                ]
                
                try:
                    result = subprocess.run(
                        cmd,
                        capture_output=True,
                        text=True,
                        timeout=5
                    )
                    
                    if result.returncode == 0:
                        data = json_module.loads(result.stdout)
                        if 'format' in data and 'duration' in data['format']:
                            duration = float(data['format']['duration'])
                            log_info("DeepfakeDetector", f"Video duration (local): {duration:.2f}s")
                            return duration
                            
                except FileNotFoundError:
                    # ffprobeä¸å­˜åœ¨ï¼Œå°è¯•ffmpeg
                    log_info("DeepfakeDetector", "ffprobe not found, trying ffmpeg")
                    
                    # æ–¹æ³•2: ä½¿ç”¨ffmpeg -i
                    ffmpeg_cmd = getattr(self, 'ffmpeg_command', 'ffmpeg')
                    cmd = [ffmpeg_cmd, '-i', actual_path, '-f', 'null', '-']
                    
                    try:
                        result = subprocess.run(
                            cmd,
                            capture_output=True,
                            text=True,
                            timeout=5
                        )
                        
                        # ä»stderrä¸­è§£æDuration
                        import re
                        duration_match = re.search(
                            r'Duration:\s*(\d{2}):(\d{2}):(\d{2}\.\d+)',
                            result.stderr
                        )
                        
                        if duration_match:
                            hours = int(duration_match.group(1))
                            minutes = int(duration_match.group(2))
                            seconds = float(duration_match.group(3))
                            duration = hours * 3600 + minutes * 60 + seconds
                            log_info("DeepfakeDetector", f"Video duration via ffmpeg: {duration:.2f}s")
                            return duration
                            
                    except FileNotFoundError:
                        log_info("DeepfakeDetector", "Neither ffprobe nor ffmpeg found")
                        
                        # æ–¹æ³•3: æœ€åå°è¯•OpenCVï¼ˆå¦‚æœå®‰è£…äº†ï¼‰
                        try:
                            import cv2
                            cap = cv2.VideoCapture(actual_path)
                            fps = cap.get(cv2.CAP_PROP_FPS)
                            frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)
                            cap.release()
                            if fps > 0 and frame_count > 0:
                                duration = frame_count / fps
                                log_info("DeepfakeDetector", f"Video duration via OpenCV: {duration:.2f}s")
                                return duration
                        except ImportError:
                            log_info("DeepfakeDetector", "OpenCV not installed")
                
                except Exception as e:
                    log_info("DeepfakeDetector", f"Error in duration detection: {e}")
            
            # æ— æ³•ç¡®å®šæ—¶é•¿
            log_info("DeepfakeDetector", "Could not determine video duration")
            return None
            
        except Exception as e:
            log_info("DeepfakeDetector", f"Error getting video duration: {e}")
            return None
    
    async def _analyze_video_keyframes_with_model(self, video_path: str, model: str = "deepfake") -> Dict[str, Any]:
        """åˆ†æè§†é¢‘å…³é”®å¸§ - æ”¯æŒä¸åŒæ¨¡å‹"""
        try:
            import subprocess
            import tempfile
            from pathlib import Path as PathLib
            
            # å¤„ç†file:IDæ ¼å¼
            actual_path = video_path
            if video_path.startswith('file:'):
                try:
                    from aicca.utils.content_loader import ContentLoader
                    loader = ContentLoader()
                    actual_path, metadata = await loader.load_content(video_path)
                except Exception as e:
                    log_info("DeepfakeDetector", f"ContentLoader failed in keyframes: {e}")
                    actual_path = video_path[5:]  # é™çº§å¤„ç†
            
            # åˆ›å»ºä¸´æ—¶ç›®å½•å­˜å‚¨æå–çš„å¸§
            with tempfile.TemporaryDirectory() as temp_dir:
                # è·å–è§†é¢‘æ—¶é•¿
                duration = await self._get_video_duration(video_path)  # è¿™ä¸ªæ–¹æ³•å†…éƒ¨å·²å¤„ç†file:ID
                
                # å¯¹äºgenaiæ£€æµ‹ï¼Œå‡å°‘å¸§æ•°ä»¥èŠ‚çœAPIè°ƒç”¨
                if model == "genai":
                    max_frames = 5  # genaiæ£€æµ‹æœ€å¤š5å¸§
                    interval = max(3, int(duration / 5)) if duration else 5
                else:
                    max_frames = 10  # deepfakeæ£€æµ‹æœ€å¤š10å¸§
                    interval = max(1, int(duration / 10)) if duration else 3
                
                log_info("DeepfakeDetector", f"Extracting max {max_frames} frames for {model} analysis")
                
                # ä½¿ç”¨ffmpegæå–å…³é”®å¸§
                output_pattern = str(PathLib(temp_dir) / "frame_%03d.jpg")
                
                cmd = [
                    'ffmpeg',
                    '-i', actual_path,
                    '-vf', f'fps=1/{interval}',
                    '-frames:v', str(max_frames),
                    '-q:v', '2',
                    output_pattern,
                    '-y'
                ]
                
                try:
                    result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
                    
                    if result.returncode != 0:
                        # å°è¯•ç®€å•æå–
                        cmd_simple = [
                            ffmpeg_cmd,
                            '-i', actual_path,
                            '-frames:v', '3',  # è‡³å°‘æå–3å¸§
                            str(PathLib(temp_dir) / "frame_%03d.jpg"),
                            '-y'
                        ]
                        result = subprocess.run(cmd_simple, capture_output=True, text=True, timeout=10)
                        
                        if result.returncode != 0:
                            return {"error": "Failed to extract frames"}
                
                except subprocess.TimeoutExpired:
                    return {"error": "Frame extraction timeout"}
                except FileNotFoundError:
                    return {"error": "ffmpeg not found"}
                
                # åˆ†ææå–çš„å¸§
                frame_files = sorted(PathLib(temp_dir).glob("frame_*.jpg"))
                
                if not frame_files:
                    return {"error": "No frames extracted"}
                
                log_info("DeepfakeDetector", f"Analyzing {len(frame_files)} frames with {model} model")
                
                # åˆ†ææ¯å¸§
                frame_results = []
                api_user = self.api_config["sightengine"]["api_user"]
                api_secret = self.api_config["sightengine"]["api_secret"]
                
                if not api_user or not api_secret:
                    return {"error": "API credentials not configured"}
                
                for i, frame_file in enumerate(frame_files[:max_frames]):
                    # è°ƒç”¨å¯¹åº”æ¨¡å‹çš„API
                    if model == "genai":
                        result = await self._call_sightengine_genai_api(str(frame_file))
                    else:
                        result = await self._call_sightengine_image_api(str(frame_file))
                    
                    if model == "genai" and "ai_generated_score" in result:
                        frame_results.append({
                            "frame": i + 1,
                            "timestamp": i * interval,
                            "ai_generated_score": result["ai_generated_score"]
                        })
                    elif model == "deepfake" and "deepfake_score" in result:
                        frame_results.append({
                            "frame": i + 1,
                            "timestamp": i * interval,
                            "deepfake_score": result["deepfake_score"]
                        })
                
                # è®¡ç®—ç»Ÿè®¡
                if frame_results:
                    if model == "genai":
                        scores = [f["ai_generated_score"] for f in frame_results]
                        return {
                            "method": "keyframe_extraction",
                            "model": "genai",
                            "frames_analyzed": len(frame_results),
                            "average_ai_score": sum(scores) / len(scores),
                            "max_ai_score": max(scores),
                            "min_ai_score": min(scores),
                            "frame_details": frame_results,
                            "note": "Analyzed for general AI-generated content"
                        }
                    else:
                        scores = [f["deepfake_score"] for f in frame_results]
                        return {
                            "method": "keyframe_extraction",
                            "model": "deepfake",
                            "frames_analyzed": len(frame_results),
                            "average_deepfake_score": sum(scores) / len(scores),
                            "max_deepfake_score": max(scores),
                            "min_deepfake_score": min(scores),
                            "frame_details": frame_results,
                            "note": "Analyzed for facial manipulation"
                        }
                else:
                    return {"error": "No valid results from frame analysis"}
            
        except Exception as e:
            return {"error": f"Frame analysis error: {str(e)}"}
    
    async def _call_sightengine_genai_api(self, image_path: str) -> Dict[str, Any]:
        """è°ƒç”¨Sightengineçš„genaiæ¨¡å‹æ£€æµ‹AIç”Ÿæˆå†…å®¹"""
        try:
            import urllib.request
            import urllib.parse
            import urllib.error
            
            api_user = self.api_config["sightengine"]["api_user"]
            api_secret = self.api_config["sightengine"]["api_secret"]
            
            if not api_user or not api_secret:
                return {"error": "API credentials not configured"}
            
            # å¤„ç†file:IDæ ¼å¼
            actual_path = image_path
            if image_path.startswith('file:'):
                try:
                    from aicca.utils.content_loader import ContentLoader
                    loader = ContentLoader()
                    actual_path, metadata = await loader.load_content(image_path)
                except Exception as e:
                    log_info("DeepfakeDetector", f"ContentLoader failed in genai_api: {e}")
                    actual_path = image_path[5:]  # é™çº§å¤„ç†
            
            # ä½¿ç”¨genaiæ¨¡å‹è€Œä¸æ˜¯deepfake
            params = {
                'models': 'genai',  # AIç”Ÿæˆå†…å®¹æ£€æµ‹
                'api_user': api_user,
                'api_secret': api_secret
            }
            
            if actual_path.startswith(('http://', 'https://')):
                params['url'] = actual_path
                data = urllib.parse.urlencode(params).encode()
                req = urllib.request.Request(
                    self.api_config["sightengine"]["endpoint"],
                    data=data
                )
                
                with urllib.request.urlopen(req, timeout=30) as response:
                    result = json.loads(response.read().decode())
            else:
                # æœ¬åœ°æ–‡ä»¶
                import requests
                with open(actual_path, 'rb') as f:
                    files = {'media': f}
                    response = requests.post(
                        self.api_config["sightengine"]["endpoint"],
                        files=files,
                        data=params,
                        timeout=30
                    )
                    result = response.json()
            
            # æå–AIç”Ÿæˆåˆ†æ•°
            if "type" in result and "ai_generated" in result["type"]:
                return {
                    "ai_generated_score": result["type"]["ai_generated"],
                    "status": result.get("status", "unknown")
                }
            else:
                return result
                
        except Exception as e:
            return {"error": f"GenAI API error: {str(e)}"}
    
    def _combine_assessments(self, deepfake_result: Dict, genai_result: Dict) -> Dict[str, Any]:
        """ç»„åˆä¸¤ä¸ªæ¨¡å‹çš„è¯„ä¼°ç»“æœ"""
        combined = {
            "facial_manipulation": False,
            "ai_generated": False,
            "overall_risk": "low"
        }
        
        # æ£€æŸ¥deepfakeç»“æœ
        if deepfake_result and "average_deepfake_score" in deepfake_result:
            if deepfake_result["average_deepfake_score"] > 0.5:
                combined["facial_manipulation"] = True
        
        # æ£€æŸ¥genaiç»“æœ
        if genai_result and "average_ai_score" in genai_result:
            if genai_result["average_ai_score"] > 0.5:
                combined["ai_generated"] = True
        
        # è®¡ç®—æ•´ä½“é£é™©
        if combined["facial_manipulation"] and combined["ai_generated"]:
            combined["overall_risk"] = "high"
        elif combined["facial_manipulation"] or combined["ai_generated"]:
            combined["overall_risk"] = "medium"
        else:
            combined["overall_risk"] = "low"
        
        return combined
    
    async def _call_video_sync_api(self, video_path: str, model: str = "deepfake") -> Dict[str, Any]:
        """è°ƒç”¨SightengineåŒæ­¥è§†é¢‘APIï¼ˆ1åˆ†é’Ÿå†…è§†é¢‘ï¼‰"""
        try:
            # å¤„ç†file:IDæ ¼å¼
            actual_path = video_path
            if video_path.startswith('file:'):
                try:
                    from aicca.utils.content_loader import ContentLoader
                    loader = ContentLoader()
                    actual_path, metadata = await loader.load_content(video_path)
                except Exception as e:
                    log_info("DeepfakeDetector", f"ContentLoader failed in call_video_sync_api: {e}")
                    actual_path = video_path[5:]  # é™çº§å¤„ç†
            
            api_user = self.api_config["sightengine"]["api_user"]
            api_secret = self.api_config["sightengine"]["api_secret"]
            
            if not api_user or not api_secret:
                return {
                    "error": "Sightengine API credentials not configured",
                    "note": "Set SIGHTENGINE_API_USER and SIGHTENGINE_API_SECRET"
                }
            
            # åŒæ­¥APIç«¯ç‚¹ï¼ˆæ ¹æ®å®˜æ–¹æ–‡æ¡£ï¼‰
            sync_endpoint = "https://api.sightengine.com/1.0/video/check-sync.json"
            
            if video_path.startswith(('http://', 'https://')):
                # URLæ–¹å¼
                import urllib.request
                import urllib.parse
                import urllib.error
                
                params = {
                    'stream_url': video_path,  # è§†é¢‘URL
                    'models': model,            # ä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹
                    'api_user': api_user,
                    'api_secret': api_secret
                }
                
                data = urllib.parse.urlencode(params).encode()
                req = urllib.request.Request(sync_endpoint, data=data)
                
                try:
                    with urllib.request.urlopen(req, timeout=120) as response:
                        result = json.loads(response.read().decode())
                    
                    # å¯é€‰ï¼šè®°å½•åŸå§‹å“åº”ä»¥ä¾¿è°ƒè¯•
                    # log_info("DeepfakeDetector", f"Sync API raw response: {json.dumps(result)[:500]}")
                    
                    # è§£æç»“æœ
                    return self._parse_video_sync_result(result)
                    
                except urllib.error.HTTPError as e:
                    error_body = e.read().decode() if e.fp else ""
                    return {
                        "error": f"HTTP {e.code}: {e.reason}",
                        "details": error_body[:200]
                    }
            else:
                # æœ¬åœ°æ–‡ä»¶ä¸Šä¼ 
                import requests
                
                with open(actual_path, 'rb') as f:
                    files = {'media': f}
                    data = {
                        'models': 'deepfake',
                        'api_user': api_user,
                        'api_secret': api_secret
                    }
                    
                    response = requests.post(
                        sync_endpoint,
                        files=files,
                        data=data,
                        timeout=120
                    )
                    
                    if response.status_code == 200:
                        result = response.json()
                        # log_info("DeepfakeDetector", f"Local file sync API response: {json.dumps(result)[:500]}")
                        return self._parse_video_sync_result(result)
                    else:
                        return {
                            "error": f"HTTP {response.status_code}",
                            "details": response.text[:200]
                        }
                        
        except Exception as e:
            log_info("DeepfakeDetector", f"Video sync API error: {e}")
            return {
                "error": f"Video sync API error: {str(e)}",
                "fallback": "Consider using keyframe analysis"
            }
    
    async def _call_video_async_api(self, video_path: str, callback_url: str) -> Dict[str, Any]:
        """è°ƒç”¨Sightengineå¼‚æ­¥è§†é¢‘APIï¼ˆè¶…è¿‡1åˆ†é’Ÿè§†é¢‘ï¼‰"""
        try:
            # å¤„ç†file:IDæ ¼å¼
            actual_path = video_path
            if video_path.startswith('file:'):
                try:
                    from aicca.utils.content_loader import ContentLoader
                    loader = ContentLoader()
                    actual_path, metadata = await loader.load_content(video_path)
                except Exception as e:
                    log_info("DeepfakeDetector", f"ContentLoader failed in call_video_async_api: {e}")
                    actual_path = video_path[5:]  # é™çº§å¤„ç†
            
            api_user = self.api_config["sightengine"]["api_user"]
            api_secret = self.api_config["sightengine"]["api_secret"]
            
            if not api_user or not api_secret:
                return {
                    "error": "API credentials not configured"
                }
            
            # å¼‚æ­¥APIç«¯ç‚¹
            async_endpoint = "https://api.sightengine.com/1.0/video/check.json"
            
            params = {
                'models': 'deepfake',
                'callback_url': callback_url,
                'api_user': api_user,
                'api_secret': api_secret
            }
            
            if actual_path.startswith(('http://', 'https://')):
                params['stream_url'] = actual_path
            else:
                # éœ€è¦ä¸Šä¼ æ–‡ä»¶
                return {
                    "note": "Async file upload requires implementation",
                    "media_id": "pending",
                    "callback_url": callback_url,
                    "status": "File upload for async processing not yet implemented"
                }
            
            import urllib.request
            import urllib.parse
            
            data = urllib.parse.urlencode(params).encode()
            req = urllib.request.Request(async_endpoint, data=data)
            
            with urllib.request.urlopen(req, timeout=30) as response:
                result = json.loads(response.read().decode())
            
            # å¼‚æ­¥APIè¿”å›media_idï¼Œç»“æœä¼šå‘é€åˆ°callback
            return {
                "media_id": result.get("media", {}).get("id"),
                "status": "processing",
                "callback_url": callback_url,
                "note": "Results will be sent to callback URL when ready"
            }
            
        except Exception as e:
            log_info("DeepfakeDetector", f"Video async API error: {e}")
            return {
                "error": f"Async API error: {str(e)}"
            }
    
    def _parse_video_sync_result(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """è§£æåŒæ­¥è§†é¢‘APIç»“æœ - åŸºäºå®é™…APIå“åº”ç»“æ„"""
        try:
            # å®é™…APIå“åº”ç»“æ„ï¼ˆæ ¹æ®curlæµ‹è¯•ï¼‰:
            # {
            #   "status": "success",
            #   "request": {...},
            #   "media": {...},
            #   "data": {
            #     "frames": [
            #       {
            #         "info": {"id": "...", "position": 0},
            #         "type": {"deepfake": 0.001}
            #       }
            #     ]
            #   }
            # }
            
            # æ£€æŸ¥å“åº”çŠ¶æ€
            if result.get("status") != "success":
                return {
                    "status": result.get("status", "error"),
                    "error": result.get("error", {}).get("message", "Unknown error"),
                    "raw_result": result
                }
            
            # è·å–framesæ•°æ® - åœ¨data.framesä¸­
            frames_data = []
            if "data" in result and "frames" in result["data"]:
                frames_data = result["data"]["frames"]
                log_info("DeepfakeDetector", f"Found {len(frames_data)} frames")
            
            # è§£ææ¯å¸§çš„deepfakeåˆ†æ•°
            deepfake_scores = []
            for i, frame in enumerate(frames_data):
                # åˆ†æ•°åœ¨ frame.type.deepfake
                if isinstance(frame, dict) and "type" in frame and "deepfake" in frame["type"]:
                    score = frame["type"]["deepfake"]
                    deepfake_scores.append(float(score))
                    
                    # å¯é€‰ï¼šè®°å½•è¯¦ç»†ä¿¡æ¯
                    # position = frame.get("info", {}).get("position", i)
                    # log_info("DeepfakeDetector", f"Frame at {position}ms: deepfake score = {score}")
            
            # è®¡ç®—ç»Ÿè®¡æ•°æ®
            if deepfake_scores:
                avg_score = sum(deepfake_scores) / len(deepfake_scores)
                max_score = max(deepfake_scores)
                min_score = min(deepfake_scores)
            else:
                avg_score = max_score = min_score = 0
                log_info("DeepfakeDetector", "No deepfake scores found in response")
            
            # è·å–è¯·æ±‚ä¿¡æ¯
            request_info = result.get("request", {})
            operations_used = request_info.get("operations", 0)
            
            return {
                "status": "success",
                "frames_analyzed": len(frames_data),
                "average_deepfake_score": avg_score,
                "max_deepfake_score": max_score,
                "min_deepfake_score": min_score,
                "frame_scores": deepfake_scores,
                "operations_used": operations_used,
                "media_id": result.get("media", {}).get("id"),
                "request_id": request_info.get("id")
            }
                
        except Exception as e:
            return {
                "error": f"Failed to parse result: {str(e)}",
                "raw_result": result
            }
    
    async def _analyze_video_keyframes(self, video_path: str) -> Dict[str, Any]:
        """åˆ†æè§†é¢‘å…³é”®å¸§è¿›è¡Œdeepfakeæ£€æµ‹ï¼ˆé™çº§æ–¹æ¡ˆï¼‰- ä½¿ç”¨ffmpegæå–å…³é”®å¸§"""
        try:
            import subprocess
            import tempfile
            from pathlib import Path as PathLib
            
            # å¤„ç†file:IDæ ¼å¼
            actual_path = video_path
            if video_path.startswith('file:'):
                try:
                    from aicca.utils.content_loader import ContentLoader
                    loader = ContentLoader()
                    actual_path, metadata = await loader.load_content(video_path)
                    log_info("DeepfakeDetector", f"Resolved file:ID to {actual_path}")
                except Exception as e:
                    log_info("DeepfakeDetector", f"ContentLoader failed in analyze_keyframes: {e}")
                    actual_path = video_path[5:]  # é™çº§å¤„ç†
            
            # åˆ›å»ºä¸´æ—¶ç›®å½•å­˜å‚¨æå–çš„å¸§
            with tempfile.TemporaryDirectory() as temp_dir:
                # æå–å…³é”®å¸§çš„ç­–ç•¥ï¼š
                # 1. æå–Iå¸§ï¼ˆå…³é”®å¸§ï¼‰
                # 2. æ¯éš”Nç§’æå–ä¸€å¸§
                # 3. é™åˆ¶æœ€å¤šåˆ†æ10å¸§ï¼ˆAPIé…é¢è€ƒè™‘ï¼‰
                
                # è·å–è§†é¢‘æ—¶é•¿ä»¥è®¡ç®—é‡‡æ ·é—´éš”
                duration = await self._get_video_duration(video_path)
                
                if duration:
                    # è®¡ç®—é‡‡æ ·é—´éš”ï¼ˆæœ€å¤š10å¸§ï¼‰
                    interval = max(1, int(duration / 10))
                    log_info("DeepfakeDetector", f"Extracting frames every {interval}s from {duration:.2f}s video")
                else:
                    # é»˜è®¤æ¯5ç§’ä¸€å¸§
                    interval = 5
                    log_info("DeepfakeDetector", "Using default 5s interval for frame extraction")
                
                # ä½¿ç”¨ffmpegæå–å…³é”®å¸§
                output_pattern = str(PathLib(temp_dir) / "frame_%03d.jpg")
                
                # ffmpegå‘½ä»¤ï¼šæå–å…³é”®å¸§
                ffmpeg_cmd = getattr(self, 'ffmpeg_command', 'ffmpeg')
                cmd = [
                    ffmpeg_cmd,
                    '-i', actual_path,              # è¾“å…¥æ–‡ä»¶
                    '-vf', f'fps=1/{interval}',    # æ¯Nç§’ä¸€å¸§
                    '-frames:v', '10',              # æœ€å¤š10å¸§
                    '-q:v', '2',                    # é«˜è´¨é‡JPEG
                    output_pattern,                 # è¾“å‡ºæ¨¡å¼
                    '-y'                            # è¦†ç›–æ–‡ä»¶
                ]
                
                # å¦‚æœæ˜¯URLï¼Œffmpegå¯ä»¥ç›´æ¥å¤„ç†
                log_info("DeepfakeDetector", f"Extracting keyframes from {'URL' if actual_path.startswith('http') else 'file'}")
                
                try:
                    result = subprocess.run(
                        cmd,
                        capture_output=True,
                        text=True,
                        timeout=30  # 30ç§’è¶…æ—¶
                    )
                    
                    if result.returncode != 0:
                        log_info("DeepfakeDetector", f"ffmpeg extraction failed: {result.stderr[:200]}")
                        
                        # å°è¯•å¤‡ç”¨æ–¹æ³•ï¼šåªæå–ç¬¬ä¸€å¸§
                        cmd_simple = [
                            ffmpeg_cmd,
                            '-i', actual_path,
                            '-frames:v', '1',
                            str(PathLib(temp_dir) / "frame_001.jpg"),
                            '-y'
                        ]
                        
                        result = subprocess.run(cmd_simple, capture_output=True, text=True, timeout=10)
                        
                        if result.returncode != 0:
                            return {
                                "error": "Failed to extract frames",
                                "details": result.stderr[:200],
                                "recommendation": "Check video format or URL accessibility"
                            }
                    
                except subprocess.TimeoutExpired:
                    return {
                        "error": "Frame extraction timeout",
                        "note": "Video might be too large or network too slow"
                    }
                except FileNotFoundError:
                    return {
                        "error": "ffmpeg not found",
                        "note": "Please install ffmpeg for video frame extraction",
                        "recommendation": "apt-get install ffmpeg or download from ffmpeg.org"
                    }
                
                # åˆ†ææå–çš„å¸§
                frame_files = sorted(PathLib(temp_dir).glob("frame_*.jpg"))
                
                if not frame_files:
                    return {
                        "error": "No frames extracted",
                        "note": "Video might be corrupted or empty"
                    }
                
                log_info("DeepfakeDetector", f"Extracted {len(frame_files)} frames for analysis")
                
                # å¯¹æ¯ä¸ªå¸§è¿›è¡Œdeepfakeæ£€æµ‹
                frame_results = []
                api_user = self.api_config["sightengine"]["api_user"]
                api_secret = self.api_config["sightengine"]["api_secret"]
                
                if not api_user or not api_secret:
                    return {
                        "frames_extracted": len(frame_files),
                        "error": "API credentials not configured for frame analysis"
                    }
                
                for i, frame_file in enumerate(frame_files[:5]):  # æœ€å¤šåˆ†æ5å¸§ä»¥èŠ‚çœAPIè°ƒç”¨
                    log_info("DeepfakeDetector", f"Analyzing frame {i+1}/{len(frame_files[:5])}")
                    
                    # è°ƒç”¨å›¾åƒdeepfake API
                    result = await self._call_sightengine_image_api(str(frame_file))
                    
                    if "deepfake_score" in result:
                        frame_results.append({
                            "frame": i + 1,
                            "timestamp": i * interval,
                            "deepfake_score": result["deepfake_score"]
                        })
                    else:
                        log_info("DeepfakeDetector", f"Frame {i+1} analysis failed: {result.get('error', 'Unknown')}")
                
                # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
                if frame_results:
                    scores = [f["deepfake_score"] for f in frame_results]
                    avg_score = sum(scores) / len(scores)
                    
                    return {
                        "method": "keyframe_extraction",
                        "frames_analyzed": len(frame_results),
                        "frames_extracted": len(frame_files),
                        "average_deepfake_score": avg_score,
                        "max_deepfake_score": max(scores),
                        "min_deepfake_score": min(scores),
                        "frame_details": frame_results,
                        "note": "Analyzed video keyframes instead of full video"
                    }
                else:
                    return {
                        "frames_extracted": len(frame_files),
                        "frames_analyzed": 0,
                        "error": "Failed to analyze extracted frames",
                        "note": "Frames were extracted but API analysis failed"
                    }
            
        except Exception as e:
            log_info("DeepfakeDetector", f"Keyframe analysis error: {e}")
            return {
                "error": f"Keyframe analysis error: {str(e)}",
                "fallback": "Unable to perform keyframe analysis"
            }
    
    async def _local_image_analysis(self, image_path: str, check_metadata: bool) -> Dict[str, Any]:
        """æœ¬åœ°å›¾åƒåˆ†æ"""
        result = {}
        
        try:
            # å¤„ç†file:IDæ ¼å¼
            actual_path = image_path
            if image_path.startswith('file:'):
                try:
                    from aicca.utils.content_loader import ContentLoader
                    loader = ContentLoader()
                    actual_path, metadata = await loader.load_content(image_path)
                except Exception as e:
                    log_info("DeepfakeDetector", f"ContentLoader failed in local_analysis: {e}")
                    actual_path = image_path[5:]  # é™çº§å¤„ç†
            
            # EXIFå…ƒæ•°æ®æ£€æŸ¥
            if check_metadata and not actual_path.startswith(('http://', 'https://')):
                from PIL import Image
                from PIL.ExifTags import TAGS
                
                image = Image.open(actual_path)
                exifdata = image.getexif()
                
                if exifdata:
                    suspicious_indicators = []
                    
                    # æ£€æŸ¥å¯ç–‘çš„ç¼–è¾‘è½¯ä»¶
                    for tag_id, value in exifdata.items():
                        tag = TAGS.get(tag_id, tag_id)
                        if tag == "Software":
                            value_str = str(value).lower()
                            if any(tool in value_str for tool in ['faceswap', 'deepface', 'gan', 'ai']):
                                suspicious_indicators.append(f"Suspicious software: {value}")
                    
                    result["metadata_check"] = {
                        "has_exif": True,
                        "suspicious_indicators": suspicious_indicators
                    }
                else:
                    result["metadata_check"] = {
                        "has_exif": False,
                        "note": "No EXIF data (could be stripped)"
                    }
            
            # å›¾åƒç»Ÿè®¡åˆ†æ
            result["statistical_analysis"] = {
                "note": "Advanced forensic analysis requires specialized libraries"
            }
            
        except Exception as e:
            result["error"] = f"Local analysis error: {str(e)}"
        
        return result
    
    async def _biological_feature_analysis(self, image_path: str) -> Dict[str, Any]:
        """ç”Ÿç‰©ç‰¹å¾åˆ†æï¼ˆçœ¨çœ¼ã€ç³å­”åå°„ç­‰ï¼‰"""
        # å¤„ç†file:IDæ ¼å¼
        actual_path = image_path
        if image_path.startswith('file:'):
            try:
                from aicca.utils.content_loader import ContentLoader
                loader = ContentLoader()
                actual_path, metadata = await loader.load_content(image_path)
            except Exception as e:
                log_info("DeepfakeDetector", f"ContentLoader failed in biological_analysis: {e}")
                actual_path = image_path[5:]  # é™çº§å¤„ç†
        
        return {
            "note": "Biological feature analysis requires face_recognition and dlib libraries",
            "features_checked": ["eye_blinking", "pupil_reflection", "facial_symmetry"],
            "status": "not_implemented"
        }
    
    async def _temporal_consistency_analysis(self, video_path: str) -> Dict[str, Any]:
        """æ—¶åºä¸€è‡´æ€§åˆ†æ"""
        try:
            # å¤„ç†file:IDæ ¼å¼
            actual_path = video_path
            if video_path.startswith('file:'):
                try:
                    from aicca.utils.content_loader import ContentLoader
                    loader = ContentLoader()
                    actual_path, metadata = await loader.load_content(video_path)
                except Exception as e:
                    log_info("DeepfakeDetector", f"ContentLoader failed in temporal: {e}")
                    actual_path = video_path[5:]  # é™çº§å¤„ç†
            
            # å°è¯•ä½¿ç”¨OpenCVè¿›è¡ŒçœŸå®åˆ†æ
            import cv2
            import numpy as np
            
            cap = cv2.VideoCapture(actual_path)
            if not cap.isOpened():
                raise Exception("Cannot open video")
            
            # æå–å¸§è¿›è¡Œåˆ†æ
            frames = []
            frame_count = 0
            max_frames = 10  # é™åˆ¶åˆ†æå¸§æ•°
            
            while len(frames) < max_frames:
                ret, frame = cap.read()
                if not ret:
                    break
                if frame_count % 30 == 0:  # æ¯30å¸§é‡‡æ ·ä¸€æ¬¡
                    frames.append(frame)
                frame_count += 1
            
            cap.release()
            
            if len(frames) < 2:
                return {
                    "status": "insufficient_frames",
                    "frames_analyzed": len(frames)
                }
            
            # è®¡ç®—å¸§é—´å·®å¼‚
            consistency_scores = []
            for i in range(1, len(frames)):
                diff = cv2.absdiff(frames[i], frames[i-1])
                score = np.mean(diff)
                consistency_scores.append(float(score))
            
            avg_consistency = np.mean(consistency_scores)
            
            return {
                "frames_analyzed": len(frames),
                "consistency_score": float(100 - min(avg_consistency, 100)),  # è½¬æ¢ä¸º0-100åˆ†æ•°
                "frame_differences": consistency_scores[:5],  # è¿”å›å‰5ä¸ªå·®å¼‚å€¼
                "status": "analyzed"
            }
            
        except ImportError:
            # OpenCVæœªå®‰è£…ï¼Œè¿”å›ç®€åŒ–ç‰ˆæœ¬
            return {
                "note": "Install opencv-python for advanced temporal analysis",
                "status": "opencv_not_available"
            }
        except Exception as e:
            # å…¶ä»–é”™è¯¯ï¼Œè¿”å›åŸå§‹å ä½ä¿¡æ¯
            return {
                "note": "Temporal consistency analysis detects frame-to-frame inconsistencies",
                "metrics": ["facial_feature_drift", "lighting_consistency", "background_stability"],
                "status": "simplified_version",
                "error": str(e)
            }
    
    async def _check_video_metadata(self, video_path: str) -> Dict[str, Any]:
        """æ£€æŸ¥è§†é¢‘å…ƒæ•°æ®"""
        log_info("DeepfakeDetector", f"[DEBUG] Starting metadata check for: {video_path}")
        try:
            # å¤„ç†file:IDæ ¼å¼
            actual_path = video_path
            if video_path.startswith('file:'):
                try:
                    from aicca.utils.content_loader import ContentLoader
                    loader = ContentLoader()
                    actual_path, metadata = await loader.load_content(video_path)
                    log_info("DeepfakeDetector", f"[DEBUG] Loaded file path: {actual_path}")
                except Exception as e:
                    log_info("DeepfakeDetector", f"[DEBUG] ContentLoader failed: {e}")
                    actual_path = video_path[5:]  # é™çº§å¤„ç†
            
            # å°è¯•ä½¿ç”¨ffmpeg-pythonè¿›è¡ŒçœŸå®å…ƒæ•°æ®åˆ†æ
            log_info("DeepfakeDetector", "[DEBUG] Trying to import ffmpeg-python")
            import ffmpeg
            import json
            log_info("DeepfakeDetector", "[DEBUG] ffmpeg-python imported successfully")
            
            probe = ffmpeg.probe(actual_path)
            log_info("DeepfakeDetector", "[DEBUG] ffmpeg.probe() executed successfully")
            
            suspicious_indicators = []
            metadata_info = {}
            
            # æå–åŸºæœ¬ä¿¡æ¯
            if 'format' in probe:
                format_info = probe['format']
                metadata_info['format'] = format_info.get('format_name', 'unknown')
                metadata_info['duration'] = float(format_info.get('duration', 0))
                metadata_info['bit_rate'] = format_info.get('bit_rate', 'unknown')
                
                # æ£€æŸ¥ç¼–ç å™¨
                if 'tags' in format_info:
                    encoder = format_info['tags'].get('encoder', '')
                    metadata_info['encoder'] = encoder
                    
                    # æ£€æŸ¥å¯ç–‘ç¼–ç å™¨
                    suspicious_encoders = ['fakeapp', 'deepfacelab', 'faceswap', 'wombo', 'reface']
                    for sus in suspicious_encoders:
                        if sus in encoder.lower():
                            suspicious_indicators.append(f"Suspicious encoder: {encoder}")
                            break
                    
                    # æ£€æŸ¥æ—¶é—´æˆ³
                    creation_time = format_info['tags'].get('creation_time', '')
                    if creation_time:
                        metadata_info['creation_time'] = creation_time
            
            # æ£€æŸ¥è§†é¢‘æµ
            if 'streams' in probe:
                for stream in probe['streams']:
                    if stream.get('codec_type') == 'video':
                        metadata_info['codec'] = stream.get('codec_name', 'unknown')
                        metadata_info['width'] = stream.get('width', 0)
                        metadata_info['height'] = stream.get('height', 0)
                        metadata_info['frame_rate'] = stream.get('r_frame_rate', 'unknown')
                        break
            
            return {
                "metadata": metadata_info,
                "suspicious_indicators": suspicious_indicators,
                "suspicious_count": len(suspicious_indicators),
                "status": "analyzed"
            }
            
        except ImportError as e:
            log_info("DeepfakeDetector", f"[DEBUG] ffmpeg-python import failed: {e}")
            # ffmpeg-pythonæœªå®‰è£…ï¼Œå°è¯•åŸºç¡€æ–¹æ³•
            try:
                import subprocess
                import json
                
                # ä½¿ç”¨å‘½ä»¤è¡Œffprobe
                ffprobe_cmd = getattr(self, 'ffmpeg_command', 'ffmpeg').replace('ffmpeg', 'ffprobe')
                log_info("DeepfakeDetector", f"[DEBUG] Using command-line ffprobe: {ffprobe_cmd}")
                
                # å¤„ç†actual_pathï¼ˆå¯èƒ½éœ€è¦ä»ä¸Šé¢çš„tryå—è·å–ï¼‰
                if 'actual_path' not in locals():
                    actual_path = video_path
                    if video_path.startswith('file:'):
                        actual_path = video_path[5:]
                
                cmd = [
                    ffprobe_cmd, '-v', 'quiet', '-print_format', 'json',
                    '-show_format', '-show_streams', actual_path
                ]
                log_info("DeepfakeDetector", f"[DEBUG] Running ffprobe command: {' '.join(cmd[:3])}...")
                result = subprocess.run(cmd, capture_output=True, text=True, timeout=10)
                log_info("DeepfakeDetector", f"[DEBUG] ffprobe returncode: {result.returncode}")
                
                if result.returncode == 0:
                    probe = json.loads(result.stdout)
                    log_info("DeepfakeDetector", "[DEBUG] ffprobe output parsed successfully")
                    # ç®€åŒ–çš„å…ƒæ•°æ®æå–
                    metadata = {
                        "format": probe.get('format', {}).get('format_name', 'unknown'),
                        "duration": probe.get('format', {}).get('duration', 'unknown'),
                        "bit_rate": probe.get('format', {}).get('bit_rate', 'unknown'),
                        "size": probe.get('format', {}).get('size', 'unknown')
                    }
                    
                    # æå–è§†é¢‘æµä¿¡æ¯
                    for stream in probe.get('streams', []):
                        if stream.get('codec_type') == 'video':
                            metadata['codec'] = stream.get('codec_name', 'unknown')
                            metadata['width'] = stream.get('width', 0)
                            metadata['height'] = stream.get('height', 0)
                            metadata['frame_rate'] = stream.get('r_frame_rate', 'unknown')
                            break
                    
                    log_info("DeepfakeDetector", f"[DEBUG] Basic metadata extracted: {metadata}")
                    return {
                        "metadata": metadata,
                        "status": "basic_analysis",
                        "note": "Using command-line ffprobe (install ffmpeg-python for advanced analysis)"
                    }
                else:
                    log_info("DeepfakeDetector", f"[DEBUG] ffprobe failed with stderr: {result.stderr[:200]}")
                
            except Exception as e2:
                log_info("DeepfakeDetector", f"[DEBUG] Command-line ffprobe failed: {e2}")
            
            # è¿”å›åŸå§‹å ä½ä¿¡æ¯
            log_info("DeepfakeDetector", "[DEBUG] Falling back to placeholder metadata")
            return {
                "note": "Install ffmpeg-python for advanced metadata analysis",
                "checks": ["creation_time", "encoder", "frame_rate", "compression_artifacts"],
                "status": "ffmpeg_not_available"
            }
            
        except Exception as e:
            # å…¶ä»–é”™è¯¯ï¼Œè¿”å›åŸºç¡€ä¿¡æ¯
            return {
                "note": "Video metadata analysis requires ffmpeg-python",
                "status": "analysis_failed",
                "error": str(e)
            }
    
    async def _call_resemble_api(self, audio_path: str) -> Dict[str, Any]:
        """è°ƒç”¨Resemble AIéŸ³é¢‘deepfakeæ£€æµ‹"""
        # å¤„ç†file:IDæ ¼å¼
        actual_path = audio_path
        if audio_path.startswith('file:'):
            try:
                from aicca.utils.content_loader import ContentLoader
                loader = ContentLoader()
                actual_path, metadata = await loader.load_content(audio_path)
            except Exception as e:
                log_info("DeepfakeDetector", f"ContentLoader failed in call_resemble_api: {e}")
                actual_path = audio_path[5:]  # é™çº§å¤„ç†
        
        return {
            "note": "Resemble AI integration requires API key",
            "free_tier": "2 free submissions available",
            "status": "not_configured",
            "audio_path": actual_path  # è¿”å›è§£æåçš„è·¯å¾„ç”¨äºåç»­å®ç°
        }
    
    async def _basic_audio_analysis(self, audio_path: str) -> Dict[str, Any]:
        """åŸºç¡€éŸ³é¢‘åˆ†æ"""
        try:
            # å¤„ç†file:IDæ ¼å¼
            actual_path = audio_path
            if audio_path.startswith('file:'):
                try:
                    from aicca.utils.content_loader import ContentLoader
                    loader = ContentLoader()
                    actual_path, metadata = await loader.load_content(audio_path)
                except Exception as e:
                    log_info("DeepfakeDetector", f"ContentLoader failed in basic_audio_analysis: {e}")
                    actual_path = audio_path[5:]  # é™çº§å¤„ç†
            
            # å°è¯•ä½¿ç”¨librosaè¿›è¡ŒçœŸå®éŸ³é¢‘åˆ†æ
            import librosa
            import numpy as np
            
            # åŠ è½½éŸ³é¢‘
            y, sr = librosa.load(actual_path, sr=None)
            
            # è®¡ç®—åŸºç¡€ç‰¹å¾
            analysis_result = {}
            
            # 1. éŸ³é¢‘é•¿åº¦
            duration = len(y) / sr
            analysis_result['duration'] = float(duration)
            
            # 2. é™éŸ³æ¯”ä¾‹
            rms = librosa.feature.rms(y=y)[0]
            silence_threshold = np.percentile(rms, 10)
            silence_ratio = np.sum(rms < silence_threshold) / len(rms)
            analysis_result['silence_ratio'] = float(silence_ratio)
            
            # 3. é¢‘è°±è´¨å¿ƒï¼ˆéŸ³è‰²ç‰¹å¾ï¼‰
            spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)[0]
            analysis_result['spectral_centroid_mean'] = float(np.mean(spectral_centroids))
            analysis_result['spectral_centroid_std'] = float(np.std(spectral_centroids))
            
            # 4. é›¶äº¤å‰ç‡ï¼ˆè¯­éŸ³/éè¯­éŸ³æ£€æµ‹ï¼‰
            zcr = librosa.feature.zero_crossing_rate(y)[0]
            analysis_result['zero_crossing_rate'] = float(np.mean(zcr))
            
            # ç®€å•çš„å¼‚å¸¸æ£€æµ‹
            suspicious_indicators = []
            
            # è¿‡äºç¨³å®šçš„é¢‘è°±è´¨å¿ƒå¯èƒ½è¡¨ç¤ºåˆæˆéŸ³é¢‘
            if analysis_result['spectral_centroid_std'] < 100:
                suspicious_indicators.append("Unusually stable spectral centroid")
            
            # è¿‡å¤šé™éŸ³å¯èƒ½è¡¨ç¤ºæ‹¼æ¥
            if silence_ratio > 0.5:
                suspicious_indicators.append("Excessive silence detected")
            
            # å¼‚å¸¸çš„é›¶äº¤å‰ç‡
            if analysis_result['zero_crossing_rate'] < 0.01 or analysis_result['zero_crossing_rate'] > 0.5:
                suspicious_indicators.append("Abnormal zero crossing rate")
            
            return {
                "audio_features": analysis_result,
                "suspicious_indicators": suspicious_indicators,
                "deepfake_risk": len(suspicious_indicators) / 3.0,  # ç®€å•é£é™©è¯„åˆ†
                "status": "analyzed"
            }
            
        except ImportError:
            # librosaæœªå®‰è£…ï¼Œè¿”å›ç®€åŒ–ç‰ˆæœ¬
            try:
                # å°è¯•ä½¿ç”¨waveåº“è·å–åŸºç¡€ä¿¡æ¯
                import wave
                
                # å¤„ç†file:IDæ ¼å¼
                actual_path = audio_path
                if audio_path.startswith('file:'):
                    try:
                        from aicca.utils.content_loader import ContentLoader
                        loader = ContentLoader()
                        actual_path, metadata = await loader.load_content(audio_path)
                    except Exception as e:
                        log_info("DeepfakeDetector", f"ContentLoader failed in audio analysis: {e}")
                        actual_path = audio_path[5:]  # é™çº§å¤„ç†
                
                with wave.open(actual_path, 'rb') as wav:
                    frames = wav.getnframes()
                    rate = wav.getframerate()
                    duration = frames / float(rate)
                    
                    return {
                        "audio_features": {
                            "duration": duration,
                            "sample_rate": rate,
                            "channels": wav.getnchannels()
                        },
                        "note": "Install librosa for advanced audio analysis",
                        "status": "basic_info"
                    }
            except Exception:
                pass
            
            # è¿”å›åŸå§‹å ä½ä¿¡æ¯
            return {
                "note": "Install librosa for audio deepfake detection",
                "metrics": ["pitch_consistency", "formant_analysis", "silence_patterns"],
                "status": "librosa_not_available"
            }
            
        except Exception as e:
            # å…¶ä»–é”™è¯¯ï¼Œè¿”å›åŸºç¡€ä¿¡æ¯
            return {
                "note": "Audio analysis failed",
                "status": "analysis_failed",
                "error": str(e)
            }
    
    def _generate_report(self, results: Dict[str, Any], media_type: str, analysis_depth: str) -> str:
        """ç”Ÿæˆæ£€æµ‹æŠ¥å‘Š"""
        report = "# Deepfake Detection Report\n\n"
        report += f"**Media Type**: {media_type}\n"
        report += f"**Analysis Depth**: {analysis_depth}\n\n"
        
        if media_type == "image":
            # Sightengineç»“æœ
            if "sightengine_analysis" in results:
                se = results["sightengine_analysis"]
                if "deepfake_score" in se:
                    score = se["deepfake_score"]
                    report += "## Deepfake Detection\n"
                    report += f"**Deepfake Score**: {score:.4f}\n"
                    report += f"*Score closer to 1 indicates higher probability of deepfake*\n\n"
                    
                    # è§£é‡Šåˆ†æ•°
                    if score < 0.3:
                        report += "**Assessment**: Low probability of deepfake\n"
                    elif score < 0.7:
                        report += "**Assessment**: Moderate probability of deepfake\n"
                    else:
                        report += "**Assessment**: High probability of deepfake\n"
                elif "error" in se:
                    report += f"**API Error**: {se['error']}\n"
                report += "\n"
            
            # æœ¬åœ°åˆ†æç»“æœ
            if "local_analysis" in results:
                local = results["local_analysis"]
                if "metadata_check" in local:
                    meta = local["metadata_check"]
                    report += "## Metadata Analysis\n"
                    if meta.get("suspicious_indicators"):
                        report += "**Suspicious Indicators Found**:\n"
                        for indicator in meta["suspicious_indicators"]:
                            report += f"- {indicator}\n"
                    else:
                        report += "No suspicious metadata found\n"
                    report += "\n"
        
        elif media_type == "video":
            report += "## Video Analysis Report\n\n"
            
            # æ·»åŠ ä½¿ç”¨çš„æ¨¡å‹ä¿¡æ¯
            if "model_used" in results:
                model = results["model_used"]
                if model == "genai":
                    report += "**Detection Type**: AI-Generated Content Detection\n"
                    report += "*Analyzing for all types of AI-generated content (synthetic scenes, objects, animations)*\n\n"
                elif model == "deepfake":
                    report += "**Detection Type**: Facial Deepfake Detection\n"
                    report += "*Analyzing specifically for face swaps and facial manipulations*\n\n"
                elif model == "comprehensive":
                    report += "**Detection Type**: Comprehensive Analysis\n"
                    report += "*Analyzing both facial deepfakes and general AI-generated content*\n\n"
            
            if "video_analysis" in results:
                va = results["video_analysis"]
                
                # GenAIæ¨¡å‹ç»“æœ
                if "average_ai_score" in va:
                    report += "### AI Content Detection Results\n"
                    report += f"**Frames Analyzed**: {va.get('frames_analyzed', 0)}\n"
                    report += f"**Average AI Score**: {va.get('average_ai_score', 0):.4f}\n"
                    report += f"**Highest AI Score**: {va.get('max_ai_score', 0):.4f}\n"
                    report += f"**Lowest AI Score**: {va.get('min_ai_score', 0):.4f}\n\n"
                    
                    # å¸§çº§åˆ«è¯¦ç»†ä¿¡æ¯
                    if "frame_details" in va and va["frame_details"]:
                        report += "**Frame-by-Frame Analysis**:\n"
                        for frame in va["frame_details"]:
                            score = frame.get('ai_generated_score', 0)
                            timestamp = frame.get('timestamp', 0)
                            frame_num = frame.get('frame', 0)
                            
                            # è¯„ä¼°ç­‰çº§
                            if score < 0.3:
                                assessment = "likely human-created"
                            elif score < 0.7:
                                assessment = "possibly AI-generated"
                            else:
                                assessment = "likely AI-generated"
                            
                            report += f"- Frame {frame_num} (@{timestamp}s): {score:.3f} *{assessment}*\n"
                    
                    # æ•´ä½“è¯„ä¼°
                    report += "\n**Overall Assessment**: "
                    avg = va.get('average_ai_score', 0)
                    if avg < 0.3:
                        report += "Low probability of AI-generated content (appears authentic)\n"
                    elif avg < 0.5:
                        report += "Mixed signals - some frames may contain AI elements\n"
                    elif avg < 0.7:
                        report += "Moderate probability of AI-generated content\n"
                    else:
                        report += "High probability of AI-generated content detected\n"
                
                # Deepfakeæ¨¡å‹ç»“æœ
                elif "average_deepfake_score" in va:
                    report += "### Facial Deepfake Detection Results\n"
                    report += f"**Frames Analyzed**: {va.get('frames_analyzed', 0)}\n"
                    report += f"**Average Deepfake Score**: {va.get('average_deepfake_score', 0):.4f}\n"
                    report += f"**Highest Score**: {va.get('max_deepfake_score', 0):.4f}\n"
                    report += f"**Lowest Score**: {va.get('min_deepfake_score', 0):.4f}\n\n"
                    
                    # å¸§çº§åˆ«è¯¦ç»†ä¿¡æ¯
                    if "frame_details" in va and va["frame_details"]:
                        report += "**Frame-by-Frame Analysis**:\n"
                        for frame in va["frame_details"]:
                            score = frame.get('deepfake_score', 0)
                            timestamp = frame.get('timestamp', 0)
                            frame_num = frame.get('frame', 0)
                            
                            if score < 0.3:
                                assessment = "no facial manipulation"
                            elif score < 0.7:
                                assessment = "possible manipulation"
                            else:
                                assessment = "likely face swap/deepfake"
                            
                            report += f"- Frame {frame_num} (@{timestamp}s): {score:.3f} *{assessment}*\n"
                    
                    # è¯„ä¼°
                    report += "\n**Overall Assessment**: "
                    avg = va.get('average_deepfake_score', 0)
                    if avg < 0.3:
                        report += "Low probability of facial deepfake\n"
                    elif avg < 0.7:
                        report += "Moderate probability of facial manipulation\n"
                    else:
                        report += "High probability of facial deepfake detected\n"
                
                # Comprehensiveæ¨¡å¼ç»“æœ
                elif "comprehensive_mode" in va:
                    report += "### Comprehensive Analysis Results\n\n"
                    
                    # é¢éƒ¨åˆ†æ
                    if "facial_analysis" in va:
                        fa = va["facial_analysis"]
                        report += "**Facial Deepfake Detection**:\n"
                        report += f"- Average Score: {fa.get('average_deepfake_score', 0):.4f}\n"
                        report += f"- Frames Analyzed: {fa.get('frames_analyzed', 0)}\n\n"
                    
                    # AIå†…å®¹åˆ†æ
                    if "general_ai_analysis" in va:
                        ga = va["general_ai_analysis"]
                        report += "**AI Content Detection**:\n"
                        report += f"- Average Score: {ga.get('average_ai_score', 0):.4f}\n"
                        report += f"- Frames Analyzed: {ga.get('frames_analyzed', 0)}\n\n"
                    
                    # ç»¼åˆè¯„ä¼°
                    if "combined_assessment" in va:
                        ca = va["combined_assessment"]
                        report += "**Combined Assessment**:\n"
                        if ca.get("facial_manipulation"):
                            report += "- Facial manipulation detected\n"
                        else:
                            report += "- No facial manipulation detected\n"
                        
                        if ca.get("ai_generated"):
                            report += "- AI-generated content detected\n"
                        else:
                            report += "- No AI-generated content detected\n"
                        
                        report += f"- Overall Risk Level: **{ca.get('overall_risk', 'unknown').upper()}**\n"
                
                # å¼‚æ­¥APIç»“æœ
                elif "media_id" in va:
                    report += f"**Status**: {va.get('status', 'processing')}\n"
                    report += f"**Media ID**: {va.get('media_id')}\n"
                    report += f"**Callback URL**: {va.get('callback_url', 'N/A')}\n"
                    report += f"*{va.get('note', 'Processing in progress')}*\n"
                
                # æ·»åŠ æŠ€æœ¯ç»†èŠ‚
                if "method" in va:
                    report += f"\n**Technical Details**:\n"
                    report += f"- Method: {va['method']}\n"
                    if "note" in va:
                        report += f"- Note: {va['note']}\n"
            
            # æ—¶åºä¸€è‡´æ€§åˆ†æ
            if "temporal_analysis" in results:
                ta = results["temporal_analysis"]
                report += "\n## Temporal Consistency Analysis\n"
                if ta.get("status") == "analyzed":
                    report += f"**Frames Analyzed**: {ta.get('frames_analyzed', 0)}\n"
                    report += f"**Consistency Score**: {ta.get('consistency_score', 0):.2f}/100\n"
                    if "frame_differences" in ta:
                        report += f"**Frame Differences**: {', '.join([f'{d:.2f}' for d in ta['frame_differences'][:3]])}\n"
                elif ta.get("status") == "opencv_not_available":
                    report += f"Note: {ta.get('note', 'OpenCV not installed')}\n"
                else:
                    report += f"Status: {ta.get('status', 'not analyzed')}\n"
                report += "\n"
            
            # å…ƒæ•°æ®åˆ†æ
            if "metadata_analysis" in results:
                ma = results["metadata_analysis"]
                report += "## Video Metadata Analysis\n"
                if ma.get("status") == "analyzed":
                    metadata = ma.get("metadata", {})
                    report += f"**Format**: {metadata.get('format', 'unknown')}\n"
                    report += f"**Duration**: {metadata.get('duration', 'unknown')} seconds\n"
                    report += f"**Resolution**: {metadata.get('width', 'unknown')}x{metadata.get('height', 'unknown')}\n"
                    report += f"**Codec**: {metadata.get('codec', 'unknown')}\n"
                    report += f"**Frame Rate**: {metadata.get('frame_rate', 'unknown')}\n"
                    report += f"**Encoder**: {metadata.get('encoder', 'unknown')}\n"
                    
                    if ma.get("suspicious_indicators"):
                        report += "\n**Suspicious Indicators**:\n"
                        for indicator in ma["suspicious_indicators"]:
                            report += f"- {indicator}\n"
                elif ma.get("status") == "basic_analysis":
                    metadata = ma.get("metadata", {})
                    report += f"**Format**: {metadata.get('format', 'unknown')}\n"
                    report += f"**Duration**: {metadata.get('duration', 'unknown')}\n"
                else:
                    report += f"Status: {ma.get('status', 'not analyzed')}\n"
                report += "\n"
        
        elif media_type == "audio":
            report += "## Audio Deepfake Analysis\n"
            report += f"*{results.get('note', '')}*\n"
            
            if "basic_analysis" in results:
                ba = results["basic_analysis"]
                report += "\n**Metrics Checked**:\n"
                for metric in ba.get("metrics", []):
                    report += f"- {metric}\n"
        
        return report
    
    def _generate_summary(self, results: Dict[str, Any], media_type: str) -> str:
        """ç”Ÿæˆç®€æ´æ‘˜è¦"""
        if media_type == "image":
            if "sightengine_analysis" in results:
                se = results["sightengine_analysis"]
                if "deepfake_score" in se:
                    score = se["deepfake_score"]
                    percentage = round(score * 100, 1)
                    return f"Deepfake detection: {score:.4f} ({percentage}% probability)"
                elif "ai_generated_score" in se:
                    score = se["ai_generated_score"]
                    percentage = round(score * 100, 1)
                    return f"AI content detection: {score:.4f} ({percentage}% probability)"
        
        elif media_type == "video":
            if "video_analysis" in results:
                va = results["video_analysis"]
                
                # GenAIæ£€æµ‹ç»“æœ
                if "average_ai_score" in va:
                    score = va["average_ai_score"]
                    percentage = round(score * 100, 1)
                    frames = va.get("frames_analyzed", 0)
                    if score > 0.7:
                        return f"High AI content probability: {percentage}% (analyzed {frames} frames)"
                    elif score > 0.5:
                        return f"Moderate AI content detected: {percentage}% (analyzed {frames} frames)"
                    else:
                        return f"Low AI content probability: {percentage}% (analyzed {frames} frames)"
                
                # Deepfakeæ£€æµ‹ç»“æœ
                elif "average_deepfake_score" in va:
                    score = va["average_deepfake_score"]
                    percentage = round(score * 100, 1)
                    frames = va.get("frames_analyzed", 0)
                    if score > 0.7:
                        return f"High deepfake probability: {percentage}% (analyzed {frames} frames)"
                    elif score > 0.3:
                        return f"Moderate deepfake risk: {percentage}% (analyzed {frames} frames)"
                    else:
                        return f"Low deepfake probability: {percentage}% (analyzed {frames} frames)"
                
                # Comprehensiveæ£€æµ‹ç»“æœ
                elif "comprehensive_mode" in va:
                    ca = va.get("combined_assessment", {})
                    risk = ca.get("overall_risk", "unknown")
                    has_facial = ca.get("facial_manipulation", False)
                    has_ai = ca.get("ai_generated", False)
                    
                    if has_facial and has_ai:
                        return f"Both facial manipulation and AI content detected - {risk.upper()} risk"
                    elif has_facial:
                        return f"Facial manipulation detected - {risk.upper()} risk"
                    elif has_ai:
                        return f"AI-generated content detected - {risk.upper()} risk"
                    else:
                        return f"No significant manipulation detected - {risk.upper()} risk"
        
        return f"Analysis completed for {media_type}"